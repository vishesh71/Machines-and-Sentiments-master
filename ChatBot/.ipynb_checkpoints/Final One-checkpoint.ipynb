{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from rake_nltk import Rake\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#nltk.download('popular', quiet=True) # for downloading packages\n",
    "#!pip3 install rake-nltk\n",
    "\n",
    "def recommend_movie_choices():\n",
    "    print(\"\\nROBO: So you want to see a new movie now right?\")\n",
    "    df_movies = pd.read_csv(\"IMDb movies.csv\")\n",
    "    df_ratings = pd.read_csv(\"IMDb ratings.csv\")\n",
    "\n",
    "    # Taking a look at all the columns in the dataframe(s)\n",
    "    #print(\"Columns for Movie Details: \\n\",df_movies.columns)\n",
    "    #print(\"Columns for Movie Rating statistics: \\n\", df_ratings.columns)\n",
    "\n",
    "    # Taking a look at the Movie Details Dataframe\n",
    "    #df_movies.head()\n",
    "\n",
    "    # Taking a look at the Movie Rating Statistics Dataframe\n",
    "    #df_ratings.head()\n",
    "\n",
    "    # Removing all the unwanted columns from the two Dataframes\n",
    "    df_movies = df_movies[['imdb_title_id','title', 'duration', 'year', 'genre', 'language', 'actors', 'director','description']]\n",
    "    df_ratings = df_ratings[['imdb_title_id', 'mean_vote', 'weighted_average_vote','median_vote', 'total_votes']]\n",
    "\n",
    "    #Again Taking a look at all the columns in the dataframe(s) after dropping unwanted columns\n",
    "    #print(\"Columns for Movie Details: \\n\",df_movies.columns)\n",
    "    #print(\"Columns for Movie Rating statistics: \\n\", df_ratings.columns)\n",
    "\n",
    "    # Merging the two dataframes and dropping all the nan values\n",
    "    df = pd.merge(df_movies, df_ratings, on='imdb_title_id')\n",
    "    #print(\"Shape, Before dropping Nan Values: \",df.shape)\n",
    "    df.dropna(inplace = True)\n",
    "    dfm = df.copy()\n",
    "    #print(\"Shape, After dropping Nan Values: \",df.shape)\n",
    "\n",
    "    df2 = df[df['language'].str.contains(r'English')]\n",
    "    #print(df2.shape)\n",
    "    df2 = df2[(df2['mean_vote'] >= 6) & (df['total_votes'] >= 1000)] # Take all English Movies with Rating greater than 6\n",
    "    #print(df2.shape)\n",
    "    df2 = df2[df2['year'] >= 1995]\n",
    "    #print(df2.shape)\n",
    "    df2[df2['title'].str.contains('123')]\n",
    "\n",
    "    df3 = df[df['language'].str.contains(r'Tamil|Kannada|Telugu|Hindi|Malayalam')]\n",
    "    #df3.shape\n",
    "    df3 = df3[(df3['mean_vote'] >= 5) & (df3['total_votes'] >= 500)]\n",
    "    df3[df3['title'].str.contains('Student')]\n",
    "    #df3.shape\n",
    "\n",
    "\n",
    "    df = pd.concat([df2,df3])\n",
    "    df = df.apply(lambda x: x.str.lower() if(x.dtype == 'O') else x)\n",
    "    df = df.drop_duplicates(subset=['title','year'], keep = False)\n",
    "\n",
    "    dfm = dfm[(dfm['mean_vote'] >= 5) & (dfm['total_votes'] >= 500)]\n",
    "    dfm = dfm.sort_values(by=['mean_vote'],ascending = False)\n",
    "    dfm = dfm.apply(lambda x: x.str.lower() if(x.dtype == 'O') else x)\n",
    "    df.shape\n",
    "    \n",
    "\n",
    "    time.sleep(3.4)\n",
    "    print(\"\\n ROBO: So to make life easy fo you I have already made a list of some of the popular movies since the 2000s in English, Hindi, Tamil and some other Indian regional languages.\")\n",
    "    print(\"ROBO: \", end='')\n",
    "    time.sleep(2.1)\n",
    "    print(\"So do you have any preferences ?\")\n",
    "    print(\"ROBO: Want to search from your own list?\")\n",
    "\n",
    "    ch = input().lower()\n",
    "    if ('yes' in ch) | ('yea' in ch) | ('ya' in ch) | ('ye' in ch):\n",
    "        # Accepting user input to identify similar movies of their interest\n",
    "        gen = input(\"ROBO: Cool!!\\nEnter Preferred genre(s) (if more than one please use a comma)(Type No if not): \").lower()\n",
    "        df2 = dfm.copy()\n",
    "        if gen != 'no':\n",
    "            df2 = dfm[dfm['genre'].str.contains(gen)]\n",
    "            df2 = df2.drop_duplicates(subset=['title'], keep = False)\n",
    "\n",
    "        lang = input(\"ROBO: Any Preferred Language(s) (if more than one please use a comma)(Type No if not): \").lower()\n",
    "        df3 = df2.copy()\n",
    "        if lang != 'no' :\n",
    "            df3 = df2[df2['language'].str.contains(lang)]\n",
    "            df3 = df3.drop_duplicates(subset = ['title'],keep=False)\n",
    "    else:\n",
    "        print(\"\\nROBO: I see you believe in me :)\")\n",
    "        df3 = df\n",
    "    df3 = df3.sort_values(by=['mean_vote'],ascending = False)\n",
    "    if df3.shape[0] > 10000:\n",
    "        df3 = df3[:10000]\n",
    "    if df3.shape[0] == 0:\n",
    "        print(\"ROBO: I'm sorry but you have not selected any movies. Please try again\")\n",
    "    df3.shape\n",
    "\n",
    "    df3.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    df3['Key_words'] = ''\n",
    "    r = Rake()\n",
    "    for index, row in df3.iterrows():\n",
    "        r.extract_keywords_from_text(row['description'])\n",
    "        key_words_dict_scores = r.get_word_degrees()\n",
    "        row['Key_words'] = list(key_words_dict_scores.keys())\n",
    "        df3['Key_words'][index] = row['Key_words']\n",
    "\n",
    "    df3['genre'] = df3['genre'].map(lambda x: x.split(','))\n",
    "    for index, row in df3.iterrows():\n",
    "        row['genre'] = [x.lower().replace(' ','') for x in row['genre']]\n",
    "\n",
    "    df3['Bag_of_words'] = ''\n",
    "    columns = ['Key_words', 'genre']\n",
    "    for index, row in df3.iterrows():\n",
    "        words = ''\n",
    "        for col in columns:\n",
    "            words += ' '.join(row[col]) + ' '\n",
    "        row['Bag_of_words'] = words\n",
    "        df3['Bag_of_words'][index] = words\n",
    "        dfn = df3[['title','Bag_of_words']]\n",
    "\n",
    "    count = CountVectorizer()\n",
    "    count_matrix = count.fit_transform(dfn['Bag_of_words'])\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    def cosine_similarity_n_space(m1, m2, batch_size=10000):\n",
    "        assert m1.shape[1] == m2.shape[1]\n",
    "        ret = np.ndarray((m1.shape[0], m2.shape[0]))\n",
    "        for row_i in range(0, int(m1.shape[0] / batch_size) + 1):\n",
    "            start = row_i * batch_size\n",
    "            end = min([(row_i + 1) * batch_size, m1.shape[0]])\n",
    "            if end <= start:\n",
    "                break # cause I'm too lazy to elegantly handle edge cases\n",
    "            rows = m1[start: end]\n",
    "            sim = cosine_similarity(rows, m2) # rows is O(1) size\n",
    "            ret[start: end] = sim\n",
    "        return ret\n",
    "\n",
    "    csmain = cosine_similarity_n_space(count_matrix, count_matrix)\n",
    "    dfn = df3[['title','Bag_of_words']]\n",
    "    count = CountVectorizer()\n",
    "    count_matrix = count.fit_transform(dfn['Bag_of_words'])\n",
    "    csmain = cosine_similarity_n_space(count_matrix, count_matrix)\n",
    "    indices = pd.Series(dfn['title'])\n",
    "    def recommend(title, num=10, cosine_sim = csmain):\n",
    "        recommended_movies = []\n",
    "        try:\n",
    "            idx = indices[indices == title].index[0]\n",
    "            score_series = pd.Series(cosine_sim[idx]).sort_values(ascending = False)\n",
    "            top_10_indices = list(score_series.iloc[1:num+1].index)\n",
    "\n",
    "            for i in top_10_indices:\n",
    "                recommended_movies.append(list(dfn['title'])[i])\n",
    "            print(\"\\nGreat Choice. Here is the list of similar movies:\")\n",
    "            for i in recommended_movies:\n",
    "                print(i.title())\n",
    "            return False\n",
    "        except:\n",
    "            print(\"ROBO: I'm sorry but I could not find such a movie in our Database.\")\n",
    "            print(\"ROBO: I'd recommend you to check the spelling of the movie you entered.\")\n",
    "            print(\"ROBO: Also make sure it belongs to the same genre and language you had entered before.\")\n",
    "            return True\n",
    "\n",
    "    ans = True\n",
    "    while (ans):\n",
    "        print(\"ROBO: Please refer to Imdb for the exact movie Name.\")\n",
    "        user_res = input(\"Enter the movie which you have in mind.\").lower()\n",
    "        num = int(input(\"How many such similar movies do you want??\"))\n",
    "        ans = recommend(user_res,num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"labeledTrainData.tsv\",sep=\"\\t\")\n",
    "data.head()\n",
    "X = data.review\n",
    "y = data.sentiment\n",
    "#Using CountVectorizer to convert text into tokens/features\n",
    "vect = CountVectorizer(stop_words='english', ngram_range = (1,1), max_df = .80, min_df = 4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1, test_size= 0.2)\n",
    "#Using training data to transform text into counts of features for each message\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "NB = MultinomialNB()\n",
    "NB.fit(X_train_dtm, y_train)\n",
    "y_pred = NB.predict(X_test_dtm)\n",
    "#print('\\nNaive Bayes')\n",
    "#print('Accuracy Score: ',metrics.accuracy_score(y_test,y_pred)*100,'%',sep='')\n",
    "#print('Confusion Matrix: ',metrics.confusion_matrix(y_test,y_pred), sep = '\\n')\n",
    "\n",
    "tokens_words = vect.get_feature_names()\n",
    "#print('\\nAnalysis')\n",
    "#print('No. of tokens: ',len(tokens_words))\n",
    "counts = NB.feature_count_\n",
    "df_table = {'Token':tokens_words,'Negative': counts[0,:],'Positive': counts[1,:]}\n",
    "tokens = pd.DataFrame(df_table, columns= ['Token','Positive','Negative'])\n",
    "positives = len(tokens[tokens['Positive']>tokens['Negative']])\n",
    "#print('No. of positive tokens: ',positives)\n",
    "#print('No. of negative tokens: ',len(tokens_words)-positives)\n",
    "#Check positivity/negativity of specific tokens\n",
    "token_search = ['horrendous']\n",
    "#print('\\nSearch Results for token/s:',token_search)\n",
    "#print(tokens.loc[tokens['Token'].isin(token_search)])\n",
    "#Analyse False Negatives (Actual: 1; Predicted: 0)(Predicted negative review for a positive review) \n",
    "#print(X_test[ y_pred < y_test ])\n",
    "#Analyse False Positives (Actual: 0; Predicted: 1)(Predicted positive review for a negative review) \n",
    "#print(X_test[ y_pred > y_test ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: Hi there\n",
      "ROBO: My name is Robo. Type 'help' for guidance, 'review' to review a movie and let others know your opinion, 'recommend' so we can recommend you a movie based on your preference :)\n",
      "Hi\n",
      "ROBO: Hi\n",
      "Recommend\n",
      "ROBO: Alright...\n",
      "\n",
      "ROBO: So you want to see a new movie now right?\n",
      "\n",
      " ROBO: So to make life easy fo you I have already made a list of some of the popular movies since the 2000s in English, Hindi, Tamil and some other Indian regional languages.\n",
      "ROBO:  So do you have any preferences ?\n",
      "ROBO: Want to make your own list?\n",
      "Yep\n",
      "ROBO: Cool!!\n",
      "Enter Preferred genre(s) (if more than one please use a comma)(Type No if not): No\n",
      "ROBO: Any Preferred Language(s) (if more than one please use a comma)(Type No if not): Tamil\n",
      "(526, 13)\n",
      "ROBO: Please refer to Imdb for the exact movie Name.\n"
     ]
    }
   ],
   "source": [
    "with open('chatbot.txt','r', encoding='utf8', errors ='ignore') as fin:\n",
    "    raw = fin.read().lower()\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"how are you\",\"hey\",\"Hi Bot\", \"Hi\")\n",
    "GREETING_RESPONSES = [\"Hi\", \"Hey\", \"*nods*\", \"Hi there\", \"Hello\",\"Hi dude, Im good\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    \"\"\"If user's input is a greeting, return a greeting response\"\"\"\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "        \n",
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response\n",
    "    \n",
    "flag=True\n",
    "print(\"ROBO: Hi there\")\n",
    "print(\"ROBO: My name is Robo. Type 'help' for guidance, 'review' to review a movie and let others know your opinion, 'recommend' so we can recommend you a movie based on your preference :)\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='recommend'):\n",
    "        if(user_response!='review'):\n",
    "            if(user_response=='thanks' or user_response=='thank you' ):\n",
    "                flag=False\n",
    "                print(\"ROBO: You are welcome..\")\n",
    "            else:\n",
    "                if(greeting(user_response)!=None):\n",
    "                    print(\"ROBO: \"+greeting(user_response))\n",
    "                else:\n",
    "                    print(\"ROBO: \",end=\"\")\n",
    "                    print(response(user_response))\n",
    "                    sent_tokens.remove(user_response)\n",
    "        else:\n",
    "            flag=False\n",
    "            print(\"ROBO: Alright...\")  \n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Alright...\")    \n",
    "        \n",
    "if(user_response=='recommend'):\n",
    "    recommend_movie_choices()\n",
    "    \n",
    "elif(user_response=='review'):\n",
    "    trainingVector = CountVectorizer(stop_words='english', ngram_range = (1,1), max_df = .80, min_df = 5)\n",
    "    trainingVector.fit(X)\n",
    "    X_dtm = trainingVector.transform(X)\n",
    "    NB_complete = MultinomialNB()\n",
    "    NB_complete.fit(X_dtm, y)\n",
    "    #Input Review\n",
    "    print('\\nTest a custom review message')\n",
    "    print('Enter review to be analysed: ', end=\" \")\n",
    "    test = []\n",
    "    test.append(input())\n",
    "    test_dtm = trainingVector.transform(test)\n",
    "    predLabel = NB_complete.predict(test_dtm)\n",
    "    tags = ['Negative','Positive']\n",
    "    #Display Output\n",
    "    print('The review is predicted:',tags[predLabel[0]])\n",
    "else:\n",
    "    print('Bye! Have a good day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag=True\n",
    "print(\"ROBO: Hi there\")\n",
    "print(\"ROBO: My name is Robo. Type 'help' for guidance, 'review' to review a movie and let others know your opinion, 'recommend' so we can recommend you a movie based on your preference :)\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='recommend'):\n",
    "        if(user_response!='review'):\n",
    "            if(user_response=='thanks' or user_response=='thank you' ):\n",
    "                flag=False\n",
    "                print(\"ROBO: You are welcome..\")\n",
    "            else:\n",
    "                if(greeting(user_response)!=None):\n",
    "                    print(\"ROBO: \"+greeting(user_response))\n",
    "                else:\n",
    "                    print(\"ROBO: \",end=\"\")\n",
    "                    print(response(user_response))\n",
    "                    sent_tokens.remove(user_response)\n",
    "        else:\n",
    "            flag=False\n",
    "            print(\"ROBO: Alright...\")  \n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Alright...\")    \n",
    "        \n",
    "if(user_response=='recommend'):\n",
    "    string = input(\"Enter a movie based on which we could recommend one to you: \")\n",
    "    print(\"Similar movies are\")\n",
    "    string = string.lower()\n",
    "    #recommend(string)\n",
    "    print(recommend(string))\n",
    "elif(user_response=='review'):\n",
    "    trainingVector = CountVectorizer(stop_words='english', ngram_range = (1,1), max_df = .80, min_df = 5)\n",
    "    trainingVector.fit(X)\n",
    "    X_dtm = trainingVector.transform(X)\n",
    "    NB_complete = MultinomialNB()\n",
    "    NB_complete.fit(X_dtm, y)\n",
    "    #Input Review\n",
    "    print('\\nTest a custom review message')\n",
    "    print('Enter review to be analysed: ', end=\" \")\n",
    "    test = []\n",
    "    test.append(input())\n",
    "    test_dtm = trainingVector.transform(test)\n",
    "    predLabel = NB_complete.predict(test_dtm)\n",
    "    tags = ['Negative','Positive']\n",
    "    #Display Output\n",
    "    print('The review is predicted:',tags[predLabel[0]])\n",
    "else:\n",
    "    print('Bye! Have a good day')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
